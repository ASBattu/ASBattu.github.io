---
layout: page
title: Communication in Human-Robot Collaboration - An Exploration of Gesture Recognition using the TIAGo Robot
description: >
  Masters' Thesis documentation
hide_description: false
sitemap: false
---

1. this list will be replaced by the toc
{:toc .large-only}


## Introduction

* This RUG Masters thesis explores communication and collaboration between humans and robots in physical collaborative transportation tasks in dynamic environments.
* The study proposes using physical gestures as a means of communication to enhance collaboration during the carrying task.


* The purpose of the study is to improve task efficiency and reliability by proposing an effective communication channel. It achieves this by presenting an innovative gesture recognition method.
* The proposed communication channel has potential applications in industries such as healthcare, manufacturing, and logistics. Or anywhere where a Robot and a Human must interact.

### Motivation
* *Goal*: Enhance communication between humans and robots collaborating in dyanamic environments.
* *Challenges with verbal and visual commands*: Using **verbal commands** meant that the robot must understand different accents of different people and know when a command starts and ends, which could be a challenge. Similarly, **visual commands** such as hand sign language meant that the robot must look at the human all the time during the collaboration, which might not be possible if the robot is navigating visually or object detecting, and also has privacy concerns.


* Explore ways to enhance communication between humans and robots in a dynamic environment. We can communicate with a robot through many channels of communication.
* Verbal commands mean the robot must understand different accents of different people, and also know when a command starts and when it ends, such as how 'Hey Siri' or 'OK Google' work in smartphones. Similarly, visual commands such as hand sign language or any other such example means that the robot must look at the human all the time during the collaboration, which might not be possible if the robot is navigating visually or object detecting, and also has privacy issues.


* This brings us to non-verbal / non-visual commands, which were chosen to be physical - gesture recognition. The motivation for this comes from watching people move objects together.

  * We drew inspiration from observing two people carrying an object together, where non-verbal communication subtly plays a more prominent role in fostering effective collaboration. Collaborating individuals might tug or release their grip on the object to signal their partner to slow down or speed up, If one person starts lowering the object, the other might deduce that their partner is tired or that this is where the object is to be set down. Similarly, one person might lead around a corner, while the other takes the lead going through a door or up the stairs. This seamless switching between the leader and follower roles and the non-verbal cues that communicate intent to the other, where both the collaborators share equal workload and trust the other when they lead, is what makes the collaboration between the two agents a true collaboration.


## Theoretical Framework
Vscode Test for github integration

## Methods

## Experimental Setup

## Results

## Conclusion